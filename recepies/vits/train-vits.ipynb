{"cells":[{"cell_type":"markdown","metadata":{"id":"XwRnOU3TTcSS"},"source":["# Install dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JxgDu46oTbj7","trusted":true},"outputs":[],"source":["!pip install -q TTS\n","!sudo apt-get -y install espeak-ng"]},{"cell_type":"markdown","metadata":{"id":"D_PWn1BnUYXr"},"source":["## Insert Dataset"]},{"cell_type":"markdown","metadata":{},"source":["You can use kaggle api to download dataset:\n","https://www.kaggle.com/datasets/magnoliasis/persian-tts-dataset-famale"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-12-23T13:21:19.931307Z","iopub.status.busy":"2022-12-23T13:21:19.930904Z","iopub.status.idle":"2022-12-23T13:21:20.876866Z","shell.execute_reply":"2022-12-23T13:21:20.875514Z","shell.execute_reply.started":"2022-12-23T13:21:19.931274Z"},"id":"UzjI324SYnSI","trusted":true},"outputs":[],"source":["!mkdir train_output"]},{"cell_type":"markdown","metadata":{},"source":["You should set your own dataset path in bellow cell like this:\n","\n","exp. : my dataset path is `/contents/persian-tts-dataset-famale` and these are files under folder:\n","```\n","/contents/persian-tts-dataset-famale\n","|-- wavs\n","|   |-- 1.wav\n","|   |-- 2.wav\n","|   |-- 3.wav\n","|   |-- ...\n","|\n","|\n","|-- metadata.csv\n","```\n","So this part of code should be (lines 26-28 in train_glowtts.py or bellow cell) like this:\n","```\n","dataset_config = BaseDatasetConfig(\n","    formatter=\"mozilla\", meta_file_train=\"metadata.csv\", path=\"/contents/persian-tts-dataset-famale\" \n",")\n","```\n"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2022-12-23T14:09:14.192412Z","iopub.status.busy":"2022-12-23T14:09:14.192050Z","iopub.status.idle":"2022-12-23T14:09:14.201899Z","shell.execute_reply":"2022-12-23T14:09:14.200714Z","shell.execute_reply.started":"2022-12-23T14:09:14.192382Z"},"id":"O3CyHQ3YTLUp","trusted":true},"outputs":[],"source":["code='''import os\n","\n","from trainer import Trainer, TrainerArgs\n","\n","from TTS.tts.configs.shared_configs import BaseDatasetConfig , CharactersConfig\n","from TTS.config.shared_configs import BaseAudioConfig\n","from TTS.tts.configs.vits_config import VitsConfig\n","from TTS.tts.datasets import load_tts_samples\n","from TTS.tts.models.vits import Vits, VitsAudioConfig\n","from TTS.tts.utils.text.tokenizer import TTSTokenizer\n","from TTS.utils.audio import AudioProcessor\n","from TTS.utils.downloaders import download_thorsten_de\n","\n","output_path = os.path.dirname(os.path.abspath(__file__))\n","dataset_config = BaseDatasetConfig(\n","    formatter=\"mozilla\", meta_file_train=\"metadata.csv\", path=\"/kaggle/input/persian-tts-dataset-famale\" \n",")\n","\n","\n","\n","audio_config = BaseAudioConfig(\n","    sample_rate=24000,\n","    do_trim_silence=True,\n","    resample=False,\n","    mel_fmin=0,\n","    mel_fmax=None \n",")\n","character_config=CharactersConfig(\n","  characters='Ø¡Ø§Ø¨ØªØ«Ø¬Ø­Ø®Ø¯Ø°Ø±Ø²Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚Ù„Ù…Ù†Ù‡ÙˆÙŠÙÙ¾Ú†Ú˜Ú©Ú¯ÛŒØ¢Ø£Ø¤Ø¥Ø¦Ù‹ÙÙÙ‘',\n","  punctuations='!(),-.:;? Ì ØŒØ›ØŸâ€Œ<>',\n","  phonemes='ËˆËŒËË‘pbtdÊˆÉ–cÉŸkÉ¡qÉ¢Ê”É´Å‹É²É³nÉ±mÊ™rÊ€â±±É¾É½É¸Î²fvÎ¸Ã°szÊƒÊ’Ê‚ÊÃ§ÊxÉ£Ï‡ÊÄ§Ê•hÉ¦É¬É®Ê‹É¹É»jÉ°lÉ­ÊÊŸaegiouwyÉªÊŠÌ©Ã¦É‘É”É™ÉšÉ›ÉÉ¨ÌƒÊ‰ÊŒÊ0123456789\"#$%*+/=ABCDEFGHIJKLMNOPRSTUVWXYZ[]^_{}',\n","  pad=\"<PAD>\",\n","  eos=\"<EOS>\",\n","  bos=\"<BOS>\",\n","  blank=\"<BLNK>\",\n","  characters_class=\"TTS.tts.utils.text.characters.IPAPhonemes\",\n","  )\n","config = VitsConfig(\n","    audio=audio_config,\n","    run_name=\"vits_fa_female\",\n","    batch_size=32,\n","    eval_batch_size=16,\n","    batch_group_size=5,\n","    num_loader_workers=0,\n","    num_eval_loader_workers=2,\n","    run_eval=True,\n","    test_delay_epochs=-1,\n","    epochs=1000,\n","    save_step=1000,\n","    text_cleaner=\"basic_cleaners\",\n","    use_phonemes=True,\n","    phoneme_language=\"fa\",\n","    characters=character_config,\n","    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n","    compute_input_seq_cache=True,\n","    print_step=25,\n","    print_eval=True,\n","    mixed_precision=False,\n","    test_sentences=[\n","        [\"Ø³Ù„Ø·Ø§Ù† Ù…Ø­Ù…ÙˆØ¯ Ø¯Ø± Ø²Ù…Ø³ØªØ§Ù†ÛŒ Ø³Ø®Øª Ø¨Ù‡ Ø·Ù„Ø®Ú© Ú¯ÙØª Ú©Ù‡: Ø¨Ø§ Ø§ÛŒÙ† Ø¬Ø§Ù…Ù‡ ÛŒ ÛŒÚ© Ù„Ø§ Ø¯Ø± Ø§ÛŒÙ† Ø³Ø±Ù…Ø§ Ú†Ù‡ Ù…ÛŒ Ú©Ù†ÛŒ \"],\n","        [\"Ù…Ø±Ø¯ÛŒ Ù†Ø²Ø¯ Ø¨Ù‚Ø§Ù„ÛŒ Ø¢Ù…Ø¯ Ùˆ Ú¯ÙØª Ù¾ÛŒØ§Ø² Ù‡Ù… Ø¯Ù‡ ØªØ§ Ø¯Ù‡Ø§Ù† Ø¨Ø¯Ø§Ù† Ø®Ùˆ Ø´Ø¨ÙˆÛŒ Ø³Ø§Ø²Ù….\"],\n","        [\"Ø§Ø² Ù…Ø§Ù„ Ø®ÙˆØ¯ Ù¾Ø§Ø±Ù‡ Ø§ÛŒ Ú¯ÙˆØ´Øª Ø¨Ø³ØªØ§Ù† Ùˆ Ø²ÛŒØ±Ù‡ Ø¨Ø§ÛŒÛŒ Ù…Ø¹Ø·Ù‘Ø± Ø¨Ø³Ø§Ø²\"],\n","        [\"ÛŒÚ© Ø¨Ø§Ø± Ù‡Ù… Ø§Ø² Ø¬Ù‡Ù†Ù… Ø¨Ú¯ÙˆÛŒÛŒØ¯.\"],\n","        [\"ÛŒÚ©ÛŒ Ø§Ø³Ø¨ÛŒ Ø¨Ù‡ Ø¹Ø§Ø±ÛŒØª Ø®ÙˆØ§Ø³Øª\"]\n","    ],\n","    output_path=output_path,\n","    datasets=[dataset_config],\n",")\n","\n","# INITIALIZE THE AUDIO PROCESSOR\n","# Audio processor is used for feature extraction and audio I/O.\n","# It mainly serves to the dataloader and the training loggers.\n","ap = AudioProcessor.init_from_config(config)\n","\n","# INITIALIZE THE TOKENIZER\n","# Tokenizer is used to convert text to sequences of token IDs.\n","# config is updated with the default characters if not defined in the config.\n","tokenizer, config = TTSTokenizer.init_from_config(config)\n","\n","# LOAD DATA SAMPLES\n","# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n","# You can define your custom sample loader returning the list of samples.\n","# Or define your custom formatter and pass it to the `load_tts_samples`.\n","# Check `TTS.tts.datasets.load_tts_samples` for more details.\n","train_samples, eval_samples = load_tts_samples(\n","    dataset_config,\n","    eval_split=True,\n","    eval_split_max_size=config.eval_split_max_size,\n","    eval_split_size=config.eval_split_size,\n",")\n","\n","# init model\n","model = Vits(config, ap, tokenizer, speaker_manager=None)\n","\n","# init the trainer and ğŸš€\n","trainer = Trainer(\n","    TrainerArgs(),\n","    config,\n","    output_path,\n","    model=model,\n","    train_samples=train_samples,\n","    eval_samples=eval_samples,\n",")\n","trainer.fit()'''\n","f=open(\"train_output/train_vits.py\",\"w\",encoding=\"utf-8\")\n","\n","f.write(code)\n","\n","f.close()"]},{"cell_type":"markdown","metadata":{"id":"HiVwNfXHYEgD"},"source":["# Start training"]},{"cell_type":"markdown","metadata":{},"source":["For training on first time run this cell: "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!PYTORCH_CUDA_ALLOC_CONF=\"max_split_size_mb:512\" python \"train_output/train_vits.py\""]},{"cell_type":"markdown","metadata":{},"source":["to continue training run this cell: \n","- set your own `continue_path`"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!PYTORCH_CUDA_ALLOC_CONF=\"max_split_size_mb:512\" python \"train_output/train_vits.py\" --continue_path 'train_output/run-December-23-2022_02+09PM-0000000'"]},{"cell_type":"markdown","metadata":{},"source":["# finetuning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!mkdir train_output/my_model"]},{"cell_type":"markdown","metadata":{},"source":["Download my last pretrained checkpoint and `config.json` :"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!wget \"https://huggingface.co/Kamtera/persian-tts-female-vits/resolve/main/checkpoint_48000.pth\" -O \"train_output/my_model/best_model.pth\"\n","!wget \"https://huggingface.co/Kamtera/persian-tts-female-vits/blob/main/config-2.json\" -O \"train_output/my_model/config.json\""]},{"cell_type":"markdown","metadata":{},"source":["You could use default settings in `config.json` or edit it with your own parameters."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!python \"train_output/train_vits.py\" --restore_path \"train_output/my_model/best_model.pth\" "]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6"}},"nbformat":4,"nbformat_minor":4}
